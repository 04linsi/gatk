\documentclass[nofootinbib,amssymb,amsmath]{revtex4}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{lmodern}
\usepackage{color}

\newcommand{\RCS}{\texttt{ReCapSeg}}
\newcommand{\ACS}{\texttt{AllelicCapSeg}}

\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}

\def\SL#1{{\color [rgb]{0,0,0.8} [SL: #1]}}
\def\DB#1{{\color [rgb]{0,0.8,0} [DB: #1]}}

\begin{document}

\title{Notes on $\ACS$}
\author{Samuel K. Lee}
\affiliation{Broad Institute, 75 Ames Street, Cambridge, MA 02142}
\email{slee@broadinstitute.org}
\date{\today}

\begin{abstract}
Some notes on the methods used in the current \texttt{python} implementation and proposed methods for the \texttt{hellbender}
port.
\end{abstract}

\maketitle



\section{Introduction}\label{introduction}

\SL{A brief intro here would be nice.}

\section{$\RCS$ Overview} \label{recapseg-overview}

We first summarize the portions of the $\RCS$ workflow that are generate the input for $\ACS$.  The below is copied/paraphrased from the documentation at \url{http://gatkforums.broadinstitute.org/discussion/5640/recapseg-overview}.

$\RCS$ is a copy-number--variant detector that runs on user-defined target regions, which can correspond to exomes, gene panels, or arbitrary windows. $\RCS$ uses a Panel of Normal (PoN) samples to model noise and normalize the coverage calls of the target sample. These methods were designed for copy-number calling (amplification and deletion) of somatic events with a resolution of two or more targets. $\RCS$ does not need a matched normal, but operates on a panel of normal samples representing similar library preparation to agnostically remove noise. $\RCS$ is the production version of the \texttt{CapSeg} algorithm.

Given an DNA-Seq alignment BAM file and a BED file of target genomic intervals, the $\RCS$ algorithms estimate copy ratio by performing the following steps:

\begin{enumerate}
\item
Generate proportional coverage: First, the per sample normalized coverage is calculated by normalizing the read coverage spanning a
target segment with the total number of aligned reads (for every read group: number of reads over segment/total number of aligned reads).  The proportional coverage is then calculated by normalizing every segment with the median normalized coverage across the PoN for the given segment.
\item
Tangent normalization: This normalization procedure projects the sample proportional coverage to a hyperplane defined by the PoN. This normalization procedure results in a copy-ratio estimate with reduced noise.
\item
Segment: The target regions are then merged into continuous segments that represent the same copy-number event. The segmentation is performed by a circular-binary-segmentation (CBS) algorithm described by Olshen et al. 2004 that was originally developed to segment noisy array copy-number data.\footnote{Specifically, the CBS implementation provided by the \texttt{R} package \texttt{DNACopy} is used. Note that even though the \texttt{DNACopy} parameter \texttt{data.type} is set to \texttt{logratio}, the tangent-normalized \emph{linear} copy ratios are used instead.  \SL{Check this?  If true, it is unclear to me how this affects the resulting segmentation, if at all.}} Currently, $\RCS$ considers only segments that include two or more targets (a target usually represents a single exon).
\end{enumerate}

To summarize, given coverage data for a set of targets, $\RCS$ produces 1) $\log_2$ copy-ratio estimates for each target, and 2) corresponding segments, which are specified by a set of genomic intervals.  The segment files produced by $\RCS$ also contain segment ``means,'' which are given by $\log_2$ of the mean linear copy ratio of all targets contained within each segment.

\section{Current $\ACS$ Workflow} \label{current-alleliccapseg-workflow}

\subsection{Segmented model} \label{segmented-model}

$\ACS$ uses allele-count data from heterozygous SNP sites to improve upon the segmented copy-number model given by $\RCS$. The $\ACS$ model is characterized by a set of segment intervals and a set of model parameters; a model with $N$ segments is taken to have $2N + 2$ parameters
\begin{equation}
\{\tau_1, f_1, \dots \tau_N, f_N, \sigma_g, \gamma\}\,.
\end{equation}
Each individual segment labeled by $i$ is specified by its true copy ratio $\tau_i$ and its true minor allele fraction $f_i$ (referred to as the ``minor homologous chromosome fraction'' in the \texttt{python} code), yielding $2N$ parameters. Two additional global parameters $\sigma_g$ and $\gamma$ determine the variance of the copy ratio and the allele-fraction bias (referred to as the ``skew''), respectively, for all segments.

Specifically, the variance of the copy ratio for targets in the $i$th segment is assumed to be given by $\sigma_i = \sigma_g \tau_i$. Furthermore, the allele-fraction bias $\gamma$ attempts to correct for a global discrepancy between the measured and true allele fractions, which may be induced by effects such as reference bias during alignment.  It is defined such that a balanced true allele fraction of $f = 1/2$ will yield, on average, an observed minor allele fraction of $\gamma/2$; we shall expand on this definition below.

\subsection{Identifying normal heterozygous SNP sites (het pulldown)} \label{identifying-normal-heterozygous-snp-sites}

The main idea behind $\ACS$ is to use allele-count data at heterozygous SNP sites to infer copy number; these sites should exhibit balanced reference/alternate read counts in the normal sample, but unbalanced copy-number events could measurably alter this balance in the tumor sample. Thus, the first step in the $\ACS$ workflow is to identify the heterozygous SNP sites that will be leveraged in the rest of the analysis.  

We first start with a list of common SNP sites, and consider all reads passing a quality-score filter (taking a default minimum quality of 0).  We examine the allele counts at these sites in the normal sample, and discard those sites that appear to be homozygous by performing a two-tailed binomial test on the counts at each site.

Specifically, for a SNP site labeled by $k$, let the reference and alternate counts be $A_k$ and $B_k$, respectively; also, let the major and minor allele counts be $M_k$ and $m_k$, respectively.  The total counts are denoted by $N_k = A_k + B_k = M_k + m_k$.\footnote{Note that the ``alternate'' and  ``minor'' counts are simply found by subtracting the reference and major counts from the total count, respectively; this lumps stray reads that do not actually match the alternate or minor alleles into both of these categories.  In principle, we could easily take only the largest and second-largest counts and discard the rest, but this is not what is currently implemented.}

For sites above a read-count threshold $N_k \geq 10$, the compatibility of the major allele count $M_k$ and the total count $N_k$ with the null hypothesis of a allele fraction $f$ (assumed to be the same across all sites, with a default value of $f = 1/2$ used) is checked with a two-tailed binomial test; a $p$-value threshold of 0.05 is assumed.  Sites with counts below the threshold or incompatible with the allele fraction $f$ (i.e., likely to be homozygous SNPs) are filtered out.

The remaining sites are assumed to be heterozygous SNPs.  The allele counts $\{A_k, B_k\}$ at these sites are pulled from the tumor sample and stored.

\subsection{Initializing the model} \label{initializing-the-model}

The initial state of the model is based on the result from $\RCS$.  The set of segment intervals is set to those found by $\RCS$, and the $\tau_i$ are set to the median copy ratio of all targets contained within each $i$th segment (note that these differ from the aforementioned segment ``means'' returned by $\RCS$, which are not used by $\ACS$).  Initial values of $\sigma_g = 0.15$ and $\gamma = 2 \times 0.48 = 0.96$ are used for the global parameters.

Initialization of the minor homologous chromosome fractions $f_i$ for each segment is more involved.  This is accomplished on a per segment basis by taking $f_i$ to be the single parameter in a beta-binomial mixture model and selecting the value that maximizes the corresponding likelihood function.  Recall that the beta-binomial distribution gives the probability for the number of successes $k$ out of a number of trials $n$, where the probability of success for each trial is a random variable that follows a beta distribution with parameters $\alpha$ and $\beta$.  The probability distribution is
\begin{equation}
p_{\beta\text{-bin}}(k | n, \alpha, \beta) = {n \choose k} \frac{B(k+\alpha, n-k+\beta)}{B(\alpha, \beta)}\,,
\end{equation}
where $B(\alpha, \beta)$ is the Euler beta function.

Consider a segment labeled by $i$, and let $D_i = \{A_{k_i}, B_{k_i}, \dots , A_{k_i+K_i}, B_{k_i+K_i}\}$ be the data set of tumor-sample allele counts for the $K_i$ heterozygous SNP sites (i.e., those identified in the het pulldown step) contained within this segment.  The likelihood function for the allele fraction $f_i$ of this segment, given the data set $D_i$, is taken to be
\begin{equation} \label{allele-frac-ll}
\mathcal{L}(f_i | D_i) = p(D_i | f_i) = 
\begin{dcases}
1\,, & D_i = \{\}\,, \\
\prod_{k = k_i}^{k_i + K_i} \left\{\frac{1}{2}(1-w)\left[\mathcal{L}_\text{ref}(f_i | A_k, B_k) + \mathcal{L}_\text{alt}(f_i | A_k, B_k)\right] + w\mathcal{L}_\text{out}\right\}\,, & \text{otherwise}\,.
\end{dcases}
\end{equation}
Here, $w$ is a weight parameter that will be later be used to account for outliers via the addition of a uniform-distribution component to the mixture model, which is represented in the likelihood by the $\mathcal{L}_\text{out}$ term; for this initialization stage, $w = 0$ is chosen.\footnote{\SL{Although, strictly speaking, $w=0$ should be used for initialization, the same value of $w = 0.0005$ used in the beta-binomial--uniform mixture model is used instead.  However, this does not have any effect on the resulting initial values of $f_i$, since the weight factor just contributes to an overall multiplicative constant in the likelihood.  Unfortunately, a further complication is that I think the likelihood for the beta-binomial--uniform mixture model is also incorrectly constructed in code; schematically, the quantity appearing under the product is coded as $1/2 (1-w)(\mathcal{L}_\textrm{ref} + \mathcal{L}_\textrm{alt}) + w' \mathcal{L}_\text{out}$, with $w' = 0.005 \neq w$.  Finally, the implementation essentially sets $\mathcal{L}_\text{out} = 1$ (albeit in a very roundabout manner).  I'm not sure if this is the desired behavior, and it also might not be consistent with how targets that are outliers in copy ratio are treated.}}

Furthermore, $\mathcal{L}_\text{ref}(f_i | A_k, B_k)$ is the likelihood of the minor allele fraction $f_i$, given allele counts $\{A_k, B_k\}$ at SNP site $k$, for the case of a reference minor allele (i.e., $A_k \leq B_k$).  It is given by
\begin{equation}
\mathcal{L}_\text{ref}(f_i | A_k, B_k) =
\begin{dcases}
1\,, & f_i = 0~\text{and}~A_i = 0\,, \\
p_{\beta\text{-bin}}(B_k | A_k+B_k, \alpha_\text{ref}(f_i), \beta_\text{ref}(f_i))\,, & \text{otherwise}\,.
\end{dcases}
\end{equation}
Here,
\begin{equation}
\alpha_\text{ref}(f_i) = \left(1 - \frac{f_i}{\gamma}\right) \Sigma\,, \qquad
\beta_\text{ref}(f_i) = \frac{f_i\Sigma}{\gamma}\,,
\end{equation}
where the parameter $\Sigma$ accounts for overdispersion and is fit to a panel of normal samples, yielding
\begin{equation}
\Sigma = \exp(s_0 + s_1\gamma/2)\,, \qquad
s_0 = -33.67232\,,\quad
s_1 = 82.77464\,.
\end{equation}

Similarly, $\mathcal{L}_\text{alt}(f_i | A_k, B_k)$ is the likelihood of $f_i$, given allele counts $\{A_k, B_k\}$ at SNP site $k$, for the case of an alternate minor allele (i.e., $B_k \leq A_k$).  It is given by
\begin{equation}
\mathcal{L}_\text{alt}(f_i | A_k, B_k) =
p_{\beta\text{-bin}}(B_k | A_k+B_k, \alpha_\text{alt}(f_i), \beta_\text{alt}(f_i))\,,
\end{equation}
with
\begin{equation}
\alpha_\text{alt}(f_i) = f_i\gamma\Sigma\,, \qquad
\beta_\text{alt}(f_i) = \left(1 - f_i\gamma\right) \Sigma\,.
\end{equation}

\SL{I'm not sure I fully understand the forms of $\alpha_\text{ref}$, $\beta_\text{ref}$, $\alpha_\text{alt}$, and $\beta_\text{alt}$ yet, but I probably just need to sit down with these equations for a bit.  Will add explanations of these terms and $\gamma$ here once I do.  Also, it would be nice to double check that the overdispersion fit is reasonable for the relevant PoNs; would be even better if we can get rid of this fit!}

In practice, the maximum-likelihood estimate for $f_i$ is found by using the \texttt{scipy} implementation of the L-BFGS-B optimization algorithm to minimize the negative log-likelihood function, requiring convergence to machine precision.  An initial guess of $f_i = 0.25$ is used, with the allowed range taken to be $10^{-10} \leq f_i \leq 0.5$.  \SL{It's not clear to me what value of $f_i$ is actually returned for the case where the segment does not contain any SNPs, $D_i= \{\}$.  It might just be the initial guess of $f_i = 0.25$, but we can check this.  If this is the case, we should check if initialization of meaningless values of $f_i$ for such segments affects things downstream.}

\subsection{Initial optimization of allelic-fraction bias} \label{initial-optimization-of-allelic-fraction-bias}

\subsection{SNP segmentation} \label{snp-segmentation}

Using the initial value of $\gamma$ found in the previous step, SNPs and their corresponding reference/alternate read counts are transformed to targets with corresponding effective ``copy ratios,'' which are then segmented using the \texttt{DNACopy} CBS implementation.  However, these effective copy ratios will not scale with the copy number as would a true copy ratio, and hence should not be thought of as such.

In particular, for a SNP site indexed by $k$ with allele counts $\{A_k, B_k\}$, the effective copy ratio $\tilde\tau_k $ is taken to be 
\begin{equation}
\tilde\tau_k =\left |\frac{B_k}{A_k + B_k} - \frac{\gamma}{2}\right|\,.
\end{equation}
The idea here is that the quantity $B_k / (A_k + B_k) - \gamma / 2$ will be clustered around zero for sites $k$ contained in segments $i$ where the true minor allele fraction is $f_i = 1/2$, but will generally form two clusters centered at $\pm$ \SL{fill in once I understand what is going on with $\gamma$}, with the positive and negative signs corresponding to sites with reference and alternate minor alleles, respectively.  By taking the effective copy ratio passed to CBS to be the absolute value of this quantity, we are simply folding the data vertically before segmenting.  \SL{Does this somehow screw up the statistics of the balanced segments by artificially reducing their variance?  Could this have an effect on CBS?}  As with the segmentation of targets in $\RCS$, the \texttt{DNACopy} parameter \texttt{data.type} is set to \texttt{logratio}.  \SL{Actually, I think that $2^{\tilde\tau_k}$ is being passed to \texttt{DNACopy}, not just $\tilde\tau_k$. Again, I'm not sure if this actually affects the segmentation, but it doesn't seem consistent with how we treat targets.  I think we should try segmenting a quantity that is proportional to copy ratio in either linear or log space for both targets and SNPs.}

\subsection{Target/SNP segment union} \label{targetsnp-segment-union}

\SL{DB can fill this in.}

\subsection{Small-segment merging} \label{small-segment-merging}

Using CBS to segment the targets in $\RCS$ results in segments that are \SL{always?} larger than $\sim$2--3 targets.  However, after taking the union of target and SNP segments, small segments with less than $\sim$2--3 targets may be introduced.  To be consistent with CBS and $\RCS$, $\ACS$ treats these small segments as spurious, and removes them by merging them with adjacent segments.

A segment is considered to be small if the number of targets it contains is strictly less than a threshold number of targets $n_t$; we take $n_t = 3$.  The small-segment merging algorithm checks each $i$th segment in turn, starting with the first, leftmost segment.  If the segment is small, it is repeatedly merged with the adjacent segment that is closer in the $L_1$ distance $|\tau_i - \tau_{i \pm 1}| + |f_i - f_{i \pm 1}|$ in copy-ratio--allele-fraction space until it is no longer small.\footnote{To be explicit, segments are reindexed after each merge, so that the new segment formed by merging segment $i$ and segment $i \pm 1$ retains the index $i$.}  Exceptions occur for adjacent segments on different chromosomes, which are never merged; in practice, this is enforced by setting the $L_1$ distance between segments on different chromosomes to be infinite.  After all segments have been checked and merged, any remaining small segments (which will be present if any chromosome contains less than $n_t$ targets) are dropped.

\subsection{Parameter optimization}\label{parameter-optimization}

\subsection{Similar-segment merging} \label{similar-segment-merging}

\subsection{Final parameter optimization} \label{final-parameter-optimization}

\section{Proposed Methods for $\texttt{hellbender}$} \label{proposed-methods-for-texttthellbender}

\subsection{Bayesian het pulldown} \label{bayesian-het-pulldown}

We are given a large data set of ref and alt read counts over many potential SNP sites and we wish to infer which sites are hets and with what probabilities.  This problem is naturally represented as a mixture model in which the hidden labels are genotypes -- hom ref, het, and hom alt.  Since the observed data are ref and alt counts is seems natural to use a binomial mixture model in which the binomial parameters are the probability of an alt read.  Then the binomial parameters are the error rate for hom ref genotypes, $1/2$ times the allelic bias for het genotypes, and $1$ minus the error rate for hom alt genotypes.  However, actual data are overdispersed because the error rate and allelic bias are random variables, not single parameters.  For example, sequencing error rates and allelic bias (for concreteness, consider mapping bias) depend on context.  Thus a beta-binomial mixture model is more appropriate.  A maximum-likelihood (MLE) approach will yield posterior probabilities on the genotypes at each locus, in particular the het probability.  It also gives the parameters of a beta distribution of allelic bias, which is useful downstream in ACS.

For generality and at no cost in complexity, consider a Dirichlet-multinomial mixture (DMM) with $K$ mixture components and $M$ classes of observed data.  For our purposes there are $K = 3$ genotypes and $M = 2$ types of read, ref and alt.  The beta-binomial distribution is the $M = 2$ case of the Dirichlet-multinomial.  The observed data are counts $n_{jm}$, the number of times class m was seen in observation $j$.  For us, each potential SNP site is a datum $j$.  Let $N_j = \sum_m n_{jm}$ denote the total read count at site $j$.  For our purposes, $\{ N_j \}$ are constants -- we are not trying to model depth of coverage here, just the proportions of the coverage allotted to ref and alt reads.

We represent hidden labels via the 1-of-$K$ encoding ${\bf z}_j = (0, 0, \ldots 1, 0, 0 \ldots)$, so $z_{jk} = 1$ when datum j comes from component $k$.  The hidden labels are multinomially distributed as $P({\bf z}_j) = {\rm Mult}({\bf z}_j | {\bf \pi})$, where $\pi_k$ is the probability of component $k$ and $\sum_k \pi_k = 1$.  Finally, the observed counts for mixture component $k$ are drawn from a Dirichlet-multinomial distribution with parameter ${\bf \alpha}_k$:

\begin{equation}
P({\bf n}_j \mid z_{jk} = 1, {\bf \alpha}_k ) = \frac{ \Gamma (A_k) }{ \Gamma (A_k + N_j) } \prod_m \frac{ \Gamma (\alpha_{km} + n_{jm}) }{ \Gamma (\alpha_{km}) },
\end{equation}
where $A_k = \sum_m \alpha_{km}$.

The EM algorithm for MLE estimates of $\{ \pi_k \}$ and $\{ \alpha_{km} \}$ requires the complete-data likelihood (CDL), that is, the joint likelihood of the observed data and hidden labels given the parameters.  In contrast, a direct approach maximizes the likelihood marginalized over the hidden variables.  The CDL of of the DMM is
\begin{align}
P({\bf z}, {\bf n} \mid {\bf \pi}, {\bf \alpha}) = & P({\bf z} \mid {\bf \pi}) P({\bf n} \mid {\bf z}, {\bf \alpha}) \\
= & \prod_{jk}  \left[ \pi_k \frac{ \Gamma (A_k) }{ \Gamma (A_k + N_j) } \prod_m \frac{ \Gamma (\alpha_{km} + n_{jm}) }{ \Gamma (\alpha_{km}) } \right]^{z_{jk}} \label{CDL}
\end{align}

In the E step of the EM algorithm, we obtain the posterior distribution on $P({\bf z} \mid {\bf n}, {\bf \pi}, {\bf \alpha})$ from Eq. (\ref{CDL}).  By inspection the posterior is a product of independent multinomials
\begin{equation}
\label{DMM_E_step}
\bar{z}_{jk} \equiv P(z_{jk} = 1 \mid {\bf n}, {\bf \pi}, {\bf \alpha})  \propto \pi_k \frac{ \Gamma (A_k) }{ \Gamma (A_k + N_j) } \prod_m \frac{ \Gamma (\alpha_{km} + n_{jm}) }{ \Gamma (\alpha_{km}) },
\end{equation}
with a normalization constant determined by the condition $\sum_k \bar{z}_{jk} = 1$.

In the M step of the EM algorithm we take the expectation of the log-CDL with respect to the posterior on ${\bf z}$ and maximize with respect to ${\bf \pi}$ and ${\bf \alpha}$.  That is, we maximize
\begin{equation}
\sum_{jk}  \bar{z}_{jk} \left\{ \log \pi_k + \log \frac{\Gamma (A_k)}{ \Gamma (A_k + N_j)}  + \sum_m \log \frac{ \Gamma (\alpha_{km} + n_{jm})}{\Gamma (\alpha_{km})}  \right\}.
\end{equation}

Maximizing with respect to $\pi_k$ with a Lagrange multiplier for the constraint $\sum_k \pi_k = 1$
\begin{equation}
\label{DMM_M_step_pi}
\pi_k = \frac{ \sum_j \bar{z}_{jk} }{\sum_{j \ell} \bar{z}_{j \ell}}
\end{equation}

To maximize with respect to ${\bf \alpha}$ we use the fact that if we are trying to maximize $f({\bf x})$ and have a current guess of ${{\bf x}_0}$, then an improved guess may be obtained by maximizing $g({\bf x})$, where $g({\bf x}_0) = f({\bf x}_0)$ and $g({\bf x}) \le f({\bf x})$ for all ${\bf x}$.  Furthermore, repeating this gives an iterative optimization that converges to a local maximum.  Using bounds (\ref{bound1}) and (\ref{bound2}) and dropping additive constants, we find that the iterative step is to maximize the lower bound
\begin{equation}
\sum_{jk}  \bar{z}_{jk} \left\{ - \left( \psi(\hat{A}_k + N_j) - \psi(\hat{A}_k) \right) A_k + \sum_m \hat{\alpha}_{km}  \left( \psi(\hat{\alpha}_{km}  + n_{jm}) - \psi(\hat{\alpha}_{km}) \right)  \log(\alpha_{km} )   \right\}.
\end{equation}
with respect to $\alpha_{km}$ treating the ``old'' guesses $\hat{\alpha}_{km}$ as constants.  This maximization is a straightforward matter of setting the derivative to zero and gives the fixed-point iteration
\begin{equation}
\label{DMMiteration}
\alpha_{km} = \hat{\alpha}_{km} \frac{\sum_j \bar{z}_{jk}  \left( \psi(\hat{\alpha}_{km}  + n_{jm}) - \psi(\hat{\alpha}_{km}) \right)} {\sum_j \bar{z}_{jk} \left( \psi(\hat{A}_k + N_j) - \psi(\hat{A}_k) \right)}
\end{equation}

As is often the case with mixture models, we risk converging to a bad local maximum if parameters are initialized poorly.  Following the approach used by Thomas Minka in his FastFit software, we obtain a good initial guess by fitting a Dirichlet mixture model (as opposed to a Dirichlet-multinomial model) on effective multinomial pseudodata.  That is, instead of working with \textit{counts} $n_{jm}$, work with \textit{proportions} $p_{jm} = n_{jm} / N_j$.  Since $\sum_m p_{jm} = 1$, ${\bf p}_j$ can be interpreted as a multinomial distribution drawn from a Dirichlet mixture.  This preprocessing step maps the original count data onto the $(M-1)$-dimensional simplex, and we can then assign the pseudo-multinomials $\{ {\bf p}_j \}$ to $K$ clusters via the $K$-means algorithm.  Define the indicator variable $\chi_{jk} = 1$ if pseudo-multinomial ${\bf p}_j$ is assigned to cluster $k$ and let $\chi_{jk} = 0$ otherwise.

We initialize $\pi_k$ as the empirical proportion of mixture component $k$ in the clustering step.  That is
\begin{equation}
\label{DMM_initialize_pi}
\pi_k = \frac{\sum_j \chi_{jk}}{N} = \frac{N_k}{N}
\end{equation}
where $N_k$ is the number of pseudo-multinomials assigned to cluster $k$.

Then for each component $k$ we initialize the Dirichlet parameter vector ${\bf \alpha_k}$ via moment matching.  Parameterize ${\bf \alpha_k}$ as ${\bf \alpha}_k = s_k {\bf \theta}_k$, where $\sum_m \theta_{km} = 1$ is the mean of the Dirichlet distribution and $s$ is its concentration.  Since multinomials $S_k = \{ {\bf p}_j \, : \, \chi_{jk} = 1 \}$ are presumed drawn from Dirichlet distribution with parameter ${\bf \alpha}_k$, we set the theoretical mean ${\bf \theta}_k$ to the empirical mean of $S_k$:
\begin{equation}
\theta_{km} = \left\langle   {\bf p}_j \in S_k \right\rangle = \frac{1}{N_k} \sum_j \chi_{jk} p_{jm}
\label{1st_moment_matching}
\end{equation}

Moment matching of the $m$-th diagonal component of the covariance gives
\begin{align}
\frac{ \alpha_{km} \left( \sum_\ell \alpha_{k \ell} - \alpha_{km} \right) }{\left( \sum_\ell \alpha_{k \ell}\right)^2 \left( \sum_\ell \alpha_{k \ell} + 1 \right) }=& {\rm cov}(S_k)_{mm} =  \left\langle   p_{jm}^2 \in S_k \right\rangle - \left\langle   p_{jm} \in S_k \right\rangle^2 \\
%
\frac{ \theta_{km} (1 - \theta_{km})} {s_k + 1} =& \frac{1}{N_k} \sum_j \chi_{jk} p^2_{jm} \\
s_k = & \frac{  \theta_{km} - \frac{1}{N_k} \sum_j \chi_{jk} p^2_{jm}}{\frac{1}{N_k} \sum_j \chi_{jk} p^2_{jm} - \theta_{km}^2 }
\label{2nd_moment_matching}
\end{align}

Since these $M$ estimates $s_k$ do not need to agree, we simply take their average.


The EM algorithm for DMM inference is summarized in Algorithm \ref{DMM}.

\begin{algorithm}
\begin{algorithmic}[1]
\State Form pseudo-multinomial data $p_{jm} = n_{jm} / N_j$
\State Find $K$ clusters of this pseudodata via the $K$-means algorithm.
\State Initialize ${\bf \pi}$ via Eq. \ref{DMM_initialize_pi}
\State Initialize $\{ \alpha_{km} \}$ via Eqs. \ref{1st_moment_matching} and \ref{2nd_moment_matching}
\Repeat
	\State Update $\bar{z}_{jk}$ via Eq. \ref{DMM_E_step}
	\State Update ${ \bf \pi}$ via Eq. \ref{DMM_M_step_pi}
		\Repeat
		\State update $ \{ \alpha_{km} \}$ via Eq. \ref{DMMiteration}
	\Until{convergence}
\Until{convergence}
\end{algorithmic}
\caption{EM algorithm for Dirichlet-multinomial mixture model}
\label{DMM}
\end{algorithm}

Returning to our original task, we obtain three mixture components with Dirichlet parameters $(\alpha_{k1}, \alpha_{k2})$.  The mean proportion of alt reads (WLOG we choose $m = 1$ to be alt and $m=2$ to be ref) are $\alpha_{k1}/(\alpha_{k1} + \alpha_{k2})$, so we can assign mixture labels $k = 1, 2, 3$ to genotypes by comparing these proportions to $0$ (hom ref), $1/2$ (het) and $1$ (hom alt). The posterior probability $\bar{z}_{jk}$ is the probability that site $j$ has genotype $k$, which is exactly what we need for a probabilistic het pulldown.

\DB{We still need to figure out how to use the beta-binomial distribution on het alt and ref counts in \ACS.}



\subsection{Model-comparison test for segment merging} \label{likelihood-based-segment-merging}

\SL{I'll fill this in once it's worked out.}

%%%APPENDICES
\appendix

\section{Finding Tight Lower Bounds on Convex and Non-convex Functions} \label{bounds}

Given a function $f(x)$ and an arbitrary value $x_0$, we seek a lower bound $g(x) \le f(x)$ that is tight at $x_0$, that is, $g(x_0) = f(x_0)$.  If $f(x)$ is convex, that is, if $f^\prime(x)$ is non-decreasing, the linearization $g(x) = f(x_0) + f^\prime(x_0)(x-x_0)$ is such a bound.  More generally, suppose $h(x) f^\prime(x)$ is non-decreasing.  Instead of approximating $f^\prime(x) \approx f^\prime(x_0)$, perhaps we can approximate $h(x) f^\prime(x) \approx h(x_0) f^\prime(x_0)$ via a candidate lower bound $g(x)$ for which

\begin{align}
g^\prime(x) =& \frac{ h(x_0) f^\prime(x_0) } {h(x)} \\
g(x) =& f(x_0) + h(x_0) f^\prime(x_0)  \int_{x_0}^x \frac{d t}{h(t)}
\end{align}

\begin{lemma}
Such a function $g(x)$ is a tight lower bound on $f(x)$ if $h(x) f^\prime(x)$ is non-decreasing for some non-negative function $h(x)$.
\end{lemma}

\begin{proof}
By the Fundamental Theorem of Calculus,

\begin{align}
f(x) - g(x) =& \int_{x_0}^{x} \left( f^\prime(t) - g^\prime(t) \right) \, dt \\
=&  \int_{x_0}^{x} \frac{ h(t) f^\prime(t) - h(x_0) f^\prime(x_0) } {h(t)} \, dt
\end{align}

By the monotonicity of $h(x)$, the integral is non-positive for $x > x_0$ and non-negative for $x < x_0$.  Either way, the resulting integral is negative, so $g(x) \le f(x)$.
\end{proof}

For Dirichlet-multinomial inference, we use the special case $h(x) = x$.

\begin{corollary} 
If $x f^\prime(x)$ is non-decreasing for $x > 0$ then for any $x_0 > 0$
\begin{equation}
f(x) \ge f(x_0) + x_0 f^\prime(x_0) \left( \log(x) - \log(x_0) \right)
\end{equation}
\end{corollary}


Two important cases are $f_1(x) = \log \Gamma(x) / \Gamma(x + n)$ and $f_2(x) = \log \Gamma(x + n) / \Gamma(x) = -f_1(x)$, where $n$ is a whole number.  Using the recursive identity $\Gamma(n+1) = n \Gamma(n)$ we have

\begin{equation}
f_1(x) = -\sum_{k=0}^{n-1} \log(x + k), \quad f^\prime_1(x) = -\sum_{k=0}^{n-1} \frac{1}{x+k}, \quad f_1^{\prime \prime}(x) = \sum_{k=0}^{n-1} \frac{1}{(x+k)^2}
\end{equation}

from which we see that $f_1(x)$ is convex and the usual linearization bound holds

\begin{align} \label{bound1}
\log \frac{ \Gamma(x) }{ \Gamma(x + n) } \ge & \log \frac{ \Gamma(x_0) }{ \Gamma(x_0 + n) } - \left( \psi(x_0 + n) - \psi(x_0) \right) (x - x_0)
\end{align}

where $\psi(x) = \frac{d}{dx} \log \Gamma(x)$ is the digamma function.  As for $f_2(x) = - f_1(x)$, it is not convex, but

\begin{equation}
x f_2^\prime(x) = \sum_{k=0}^{n-1} \frac{x}{x+k} = \sum_{k=0}^{n-1} \frac{1}{1+k/x},
\end{equation}

which is increasing since each denominator is decreasing.  Thus we apply the above corollary to obtain

\begin{equation} \label{bound2}
\log \frac{ \Gamma(x + n) }{ \Gamma(x) } \ge \log \frac{ \Gamma(x_0 + n) }{ \Gamma(x_0) } + x_0 \left( \psi(x_0 + n) - \psi(x_0) \right)  \left( \log(x) - \log(x_0) \right)
\end{equation}
\end{document}
